# -*- coding: utf-8 -*-
"""dim_reduction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tbo0jTHiCAHZjunaXkTXdnX5lMHTfZcF
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

def RF(X_train, y_train, X_test, n_estimators, g):
  # create the classifier
  clf = RandomForestClassifier(n_estimators)

  # train the classifier
  clf.fit(X_train, y_train.values.ravel())

  # feature importance (gini)
  feature_imp = pd.Series(clf.feature_importances_,index = X_train.columns).sort_values(ascending=False)

  # plot
  plot = sns.barplot(x = feature_imp, y = feature_imp.index) # TO DO ! eliminate y axis legends
  plt.xlabel('Feature Importance Score')
  plt.ylabel('Features')
  plt.title("Visualizing Important Features")
  plt.show()

  # select most important features
  s = feature_imp > g
  selected_features = list((s[s]).index)
  print("N selected features: " + str(len(selected_features)))

  X_train = X_train[selected_features]
  X_test = X_test[selected_features]

  return X_train, X_test


def myPCA(X, nc):
  # scale the features
  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  # plot explained variance to choose num components
  pca = PCA()
  pca.fit_transform(X)
  pca_variance = pca.explained_variance_

  plt.figure(figsize=(8, 6))
  plt.bar(range(162), pca_variance, alpha=0.5, align='center', label='individual variance')
  plt.legend()  
  plt.ylabel('Variance ratio')
  plt.xlabel('Principal components')
  plt.show()

  # apply transformation
  pca2 = PCA(n_components=nc)
  pca2.fit(X)
  X_red = pca2.transform(X)
  X_red = pd.DataFrame(X_red)

  return X_red